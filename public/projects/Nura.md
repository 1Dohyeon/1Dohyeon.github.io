# Nura: RAG 기반 AI 문서 분석 및 질의응답 시스템

이 문서는 프로젝트를 시작한 이유부터 배운 점까지 프로젝트 경험을 중심으로 작성되었습니다. **코드 구현 세부사항, 아키텍처 설계, 사용 방법 등** 기술 문서는 [GitHub README](https://github.com/newlearnnote/Nura-server/blob/main/README.md)에서 확인하실 수 있습니다.

## 프로젝트 개요

> 프로젝트 정보
>
> - 프로젝트명: Nura
> - 목적: NewLearn Note에 통합될 AI 문서 분석 기능의 독립 프로토타입 구축 및 RAG 파이프라인 검증
> - 개발 기간: 2025.12
> - 개발 인원: 1인 (개인 프로젝트)

### 왜 이 프로젝트를 시작했는가?

NewLearn Note의 핵심 기능 중 하나는 사용자가 작성한 노트를 AI가 분석하고 질문에 답변하는 것입니다. 하지만 메인 프로젝트에 바로 AI 기능을 통합하기에는 다음과 같은 우려가 있었습니다:

- **기술적 불확실성**: RAG 파이프라인의 각 단계(문서 파싱, 청킹, 임베딩, 검색, 생성)를 처음 다뤄보는 것이라 어떤 문제가 생길지 예측하기 어려웠습니다.
- **성능 검증 필요**: 문서 크기에 따른 응답 속도, 청킹 전략에 따른 답변 정확도 등을 미리 테스트해야 했습니다.
- **독립적인 실험 환경**: 메인 프로젝트에 영향을 주지 않고 자유롭게 실험하고 싶었습니다.

이러한 이유로 Nura를 독립 프로젝트로 개발하여 RAG 파이프라인을 먼저 검증하고, 이후 NewLearn Note에 안정적으로 통합하는 전략을 선택했습니다.

## 주요 개발 경험과 기술적 의사결정

### 1. FastAPI 선택: 빠른 프로토타입 개발

**직면한 선택:**
백엔드 프레임워크로 두 가지 옵션이 있었습니다:
- **Option A**: NestJS - 메인 프로젝트와 동일한 스택, 하지만 Python AI 라이브러리 연동 복잡
- **Option B**: FastAPI - Python 네이티브 지원, AI 라이브러리 생태계 활용 용이

**선택 이유:**
1. **Python AI 생태계 활용**: LangChain, ChromaDB, OpenAI API 등이 모두 Python 기반
2. **빠른 개발 속도**: 프로토타입 검증이 목적이므로 개발 속도 우선
3. **비동기 처리 지원**: FastAPI의 네이티브 async/await로 문서 처리 성능 향상

**학습:**
프로젝트의 목적에 따라 익숙한 스택보다 적합한 스택을 선택하는 것이 더 효율적일 수 있다는 것을 배웠습니다.

### 2. RAG 파이프라인 아키텍처 설계

**직면한 문제:**
처음에는 "문서를 업로드하면 AI가 답변한다"는 단순한 아이디어만 있었습니다. 하지만 실제로 구현하려니 다음과 같은 질문들이 생겼습니다:
- 문서를 어떻게 의미 있는 단위로 나눌 것인가? (청킹)
- 사용자 질문과 관련된 문서 부분을 어떻게 찾을 것인가? (검색)
- 검색된 문서를 AI에게 어떻게 전달할 것인가? (프롬프트)

**해결 과정:**
1. **사용자 경험 설계**
   - VSCode의 Copilot UI를 참고하여 직관적인 문서 업로드 방식 채택
   - 사용자가 PDF, Markdown, Text 파일을 드래그 앤 드롭으로 업로드(Newlearn Note 프로젝트에 도입 예정)
   - 복잡한 검색 과정 없이 바로 질문할 수 있도록 UX 단순화

2. **문서 파싱 및 청킹 전략**
   - PDF, Markdown, Text 파일 형식 지원
   - 초기: 고정 길이 청킹 (500자) → 문장이 중간에 잘리는 문제
   - 개선: 문단/섹션 단위 청킹으로 의미 보존
   - 고민: 너무 작으면 맥락 손실, 너무 크면 검색 정확도 하락

3. **벡터 검색 시스템 구축**
   - ChromaDB를 벡터 저장소로 선택
   - 이유: 경량화되어 있고 Python 통합이 쉬움
   - 문서 청크를 임베딩하여 벡터로 변환 후 저장
   - 사용자 질문도 임베딩하여 유사도 기반 검색

4. **프롬프트 엔지니어링**
   - 검색된 문서 청크를 컨텍스트로 제공
   - 시스템 프롬프트로 AI의 답변 스타일 정의
   - "문서에 없는 내용은 답변하지 않도록" 제약 추가

**구현 결과:**

사용자 문서 업로드
    ↓
파싱 (PDF/MD/TXT)
    ↓
청킹 (의미 단위 분할)
    ↓
임베딩 (벡터 변환)
    ↓
ChromaDB 저장
    ↓
사용자 질문 입력
    ↓
질문 임베딩
    ↓
유사도 검색 (Top-K)
    ↓
관련 문서 추출
    ↓
프롬프트 생성
    ↓
OpenAI API 호출
    ↓
답변 반환


**학습:**
RAG는 단순히 "AI에게 문서를 던지는 것"이 아니라, 문서 전처리-검색-생성의 각 단계를 최적화해야 하는 파이프라인이라는 것을 배웠습니다. 특히 청킹 전략이 전체 시스템 성능에 큰 영향을 미친다는 점이 인상적이었습니다.

### 3. 비동기 문서 처리 구현

**직면한 문제:**
대용량 PDF 파일(예: 100페이지 논문)을 업로드할 때 처리 시간이 오래 걸려 사용자가 기다려야 하는 문제가 있었습니다.

**해결 과정:**
1. **동기 처리의 한계 파악**
   - 100페이지 PDF 처리 시 약 30초 소요
   - 이 동안 사용자는 아무것도 할 수 없음

2. **비동기 처리 구현**
   - FastAPI의 BackgroundTasks 활용
   - 파일 업로드 즉시 응답 반환 (작업 ID 제공)
   - 백그라운드에서 문서 파싱 및 임베딩 진행
   - 상태 확인 API로 진행 상황 조회 가능

3. **에러 처리**
   - 비동기 작업 실패 시 사용자에게 알림
   - 실패한 문서는 재시도 가능하도록 설계

**학습:**
사용자 경험을 고려한 비동기 처리의 중요성을 배웠습니다. 특히 시간이 오래 걸리는 작업은 백그라운드에서 처리하고 상태를 추적할 수 있도록 하는 것이 중요하다는 점을 깨달았습니다.

### 4. LangChain 통합과 프롬프트 최적화

**직면한 문제:**
OpenAI API를 직접 호출할 수도 있었지만, 문서 처리, 프롬프트 관리, 체인 구성 등을 일일이 구현하는 것은 비효율적이었습니다.

**해결 과정:**
1. **LangChain 도입 결정**
   - 문서 로더, 텍스트 스플리터, 임베딩 등 통합된 인터페이스 제공
   - RetrievalQA 체인으로 RAG 파이프라인 간단히 구성
   - 프롬프트 템플릿 관리 용이

2. **프롬프트 엔지니어링**
   - 초기: 단순히 "이 문서를 읽고 질문에 답해줘"
   - 문제: 문서와 관련 없는 질문에도 문서 내용을 억지로 끼워 맞춰 답변하는 경우 발생
   - 개선: "주어진 문서와 관련된 질문이면 문서 내용을 우선 참고하고, 관련 없는 질문이면 AI의 일반 지식으로 유연하게 답변"
   - 목표: 문서 기반 답변과 일반 대화를 자연스럽게 전환할 수 있는 유연한 AI 어시스턴트 구현

**학습:**
프레임워크를 잘 활용하면 개발 속도를 크게 높일 수 있다는 것을 배웠습니다. 특히 LangChain처럼 잘 설계된 추상화는 복잡한 파이프라인을 단순하게 만들어준다는 점이 인상적이었습니다. 또한 프롬프트 엔지니어링이 AI 시스템의 품질에 결정적인 영향을 미친다는 것을 경험했습니다.

### 5. Google Cloud Storage 연동

**직면한 선택:**
업로드된 문서를 어디에 저장할 것인가?
- **Option A**: 로컬 파일 시스템 - 간단하지만 서버 재시작 시 손실 위험
- **Option B**: GCS (Google Cloud Storage) - 영구 저장, NewLearn Note와 통합 용이

**선택 이유:**
1. **데이터 영속성**: 서버 재시작이나 재배포 시에도 문서 보존
2. **NewLearn Note 통합 고려**: 메인 프로젝트도 GCS 사용 중이므로 통합 용이
3. **확장성**: 향후 여러 서버에서 동일한 문서 접근 가능

**구현 내용:**
- Python GCS 클라이언트 라이브러리 활용
- 파일 업로드 시 GCS에 저장 후 경로 DB에 기록
- 문서 분석 시 GCS에서 다운로드하여 처리

**학습:**
프로토타입이라도 데이터 영속성과 향후 통합을 고려한 아키텍처 설계가 중요하다는 것을 배웠습니다.

## 프로젝트를 통해 배운 점

### 기술적 학습

1. **RAG 파이프라인 구축 경험**
   - LangChain, ChromaDB 등 오픈소스 라이브러리를 조합하여 RAG 시스템 구축
   - 문서 전처리, 벡터화, 검색, 생성의 전체 흐름 경험
   - 각 단계의 최적화 포인트 파악 (특히 청킹 전략)

2. **LLM API 활용**
   - OpenAI API를 활용한 실용적인 AI 서비스 구현
   - 프롬프트 엔지니어링의 중요성 체감

3. **Python 백엔드 개발**
   - FastAPI로 RESTful API 설계 및 구현
   - 비동기 처리 패턴 학습

### 프로젝트 관리 학습

1. **프로토타입 우선 접근**
   - 메인 프로젝트에 바로 통합하지 않고 독립적으로 검증
   - 실패 비용을 낮추고 빠르게 학습할 수 있었음

2. **기술 스택 선택의 유연성**
   - 익숙한 NestJS 대신 프로젝트에 적합한 FastAPI 선택
   - 도구는 목적에 맞게 선택해야 한다는 교훈

### 다음 단계

Nura에서 검증한 RAG 파이프라인을 NewLearn Note에 통합할 계획입니다. 특히 다음 사항들을 고려하고 있습니다:

- **프론트엔드 통합**: Next.js 웹/Electron 데스크톱 앱에서 AI 어시스턴스 UI 구현
- **성능 최적화**: 실시간 질의응답을 위한 캐싱 전략 수립
- **비용 관리**: OpenAI API 호출 비용 최적화 방안 모색

이 프로젝트는 단순히 AI 기능을 구현하는 것을 넘어, RAG 시스템의 작동 원리를 깊이 이해하고 실용적인 AI 서비스를 만드는 방법을 배우는 소중한 경험이었습니다.